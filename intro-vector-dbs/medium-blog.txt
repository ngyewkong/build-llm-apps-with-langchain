VectorDB Tutorial — A Beginner’s Guide
A Simple Tutorial on VectorDB with Code
Krishna yogi
Krishna yogi

·
Follow

6 min read
·
Jul 18, 2024
23




A Vector Database (VectorDB) is designed to store and manage vector data, often used in machine learning and AI applications. Vector data refers to numerical representations of objects, which can be used for similarity search, clustering, and other tasks.


Image Source: Langchain
Core Concepts of VectorDB
Vector Representations:

Vectors are numerical representations of objects, often derived from feature extraction processes like embedding models in machine learning.
Vectors can be of high-dimensionality, capturing various attributes of the objects they represent.
Similarity Search:

The core functionality of a VectorDB is to perform similarity search efficiently.
Similarity between vectors is typically measured using distance metrics like Euclidean (L2) distance, Cosine similarity, or Manhattan (L1) distance.
Nearest Neighbor Search (NNS) algorithms are used to find the most similar vectors to a given query vector.
Indexing:

Indexing is the process of organizing vectors to enable fast and efficient retrieval.
Common indexing techniques include KD-trees, Ball trees, and more advanced methods like Locality-Sensitive Hashing (LSH) and Approximate Nearest Neighbor (ANN) algorithms such as HNSW (Hierarchical Navigable Small World) and FAISS (Facebook AI Similarity Search).
Dimensionality Reduction:

Techniques like Principal Component Analysis (PCA) and t-SNE are used to reduce the number of dimensions in vectors while preserving their significant properties.
This helps in speeding up the search and reducing storage requirements.
VectorDB Architecture
Data Ingestion Layer:

Handles the input of raw data and processes it into vector representations using feature extraction models like neural networks.
Ensures data is normalized and transformed appropriately before indexing.
Indexing Layer:

Responsible for creating and maintaining the index of vectors.
Employs various indexing algorithms to balance between accuracy and performance.
Storage Layer:

Stores the vectors and the indices.
Can be designed for both in-memory storage for fast access and disk-based storage for handling large datasets.
Query Processing Layer:

Manages the execution of similarity search queries.
Utilizes the index to quickly retrieve the nearest neighbors to the query vector.
Often includes components for filtering, ranking, and aggregating search results.
API Layer:

Provides interfaces for interacting with the VectorDB.
Includes functionalities for adding, updating, deleting vectors, and executing search queries.
Use Cases of VectorDB
Recommendation Systems:

Powering personalized recommendations by finding similar users or items based on their vector representations.
Examples: Movie, music, or product recommendations.
Image and Video Retrieval:

Enabling content-based image and video retrieval by searching for similar visual features.
Examples: Google Images, Pinterest visual search.
Natural Language Processing (NLP):

Finding semantically similar texts, documents, or queries using vector embeddings from models like BERT or Word2Vec.
Examples: Document clustering, sentiment analysis, semantic search.
Anomaly Detection:

Identifying unusual patterns or outliers in high-dimensional data.
Examples: Fraud detection, network intrusion detection.
Biometric Identification:

Matching biometric data such as fingerprints, facial recognition, or iris scans by comparing vectorized features.
Examples: Security systems, user authentication.
Genomics and Bioinformatics:

Analyzing genetic sequences and biological data by comparing vector representations of DNA, RNA, or protein sequences.
Examples: Disease research, personalized medicine.
Example Technologies and Tools
FAISS (Facebook AI Similarity Search):

A library for efficient similarity search and clustering of dense vectors.
Supports a range of indexing algorithms and is optimized for both CPU and GPU.
Annoy (Approximate Nearest Neighbors Oh Yeah):

A library for approximate nearest neighbor search, particularly useful for large, read-only datasets.
Uses a combination of random projections and trees for indexing.
HNSW (Hierarchical Navigable Small World):

An algorithm for efficient approximate nearest neighbor search.
Utilizes a graph-based structure for fast and scalable search.
Milvus:

An open-source vector database designed for scalable similarity search.
Supports hybrid search combining vector and traditional databases.
Steps to Use VectorDB
1. Install the Required Libraries
You’ll need a library like faiss (Facebook AI Similarity Search) or Annoy (Approximate Nearest Neighbors Oh Yeah) to work with vectors. Let's use faiss for this example.

pip install faiss-cpu
2. Import the Library
import faiss
import numpy as np
3. Create Some Vector Data
Let’s create some sample vectors.

# Generate random vectors
d = 128  # Dimension of vectors
nb = 1000  # Number of vectors
np.random.seed(1234)
vectors = np.random.random((nb, d)).astype('float32')
4. Build the Index
We’ll build an index to store these vectors and make similarity searches efficient.

index = faiss.IndexFlatL2(d)  # L2 distance
index.add(vectors)  # Add vectors to the index
print(f"Number of vectors in the index: {index.ntotal}")
5. Perform a Similarity Search
Now, let’s search for the top 5 vectors similar to a query vector.

# Generate a random query vector
query_vector = np.random.random((1, d)).astype('float32')
# Perform the search
k = 5  # Number of nearest neighbors
distances, indices = index.search(query_vector, k)
print("Nearest neighbors:")
print(indices)
print("Distances:")
print(distances)
6. Advanced Usage: Adding and Removing Vectors
You can add more vectors to the index or remove vectors if needed.

# Add more vectors
more_vectors = np.random.random((500, d)).astype('float32')
index.add(more_vectors)

# Remove vectors by their indices (only supported in some index types)
# Example with an ID-mapped index
index_with_ids = faiss.IndexIDMap(index)
index_with_ids.add_with_ids(more_vectors, np.arange(nb, nb + 500))

# To remove vectors, create a new index without the unwanted vectors
indices_to_remove = np.array([0, 1, 2], dtype='int64')
mask = np.isin(np.arange(index.ntotal), indices_to_remove, invert=True)
vectors_to_keep = vectors[mask]
index = faiss.IndexFlatL2(d)
index.add(vectors_to_keep)
Output
Here is a sample output that you can see when you execute each step:

Initial number of vectors: 1000
Nearest neighbors for a query vector: Indices [[729 594 134 349 870]], Distances [[17.123356 17.326893 17.376125 17.495504 17.497896]]
Number of vectors after adding more: 1500
Number of vectors after removal: 997
You’ve now learned the basics of using a Vector Database with faiss. This tutorial covered creating vectors, building an index, and performing similarity searches. You can extend this knowledge to more complex tasks and larger datasets.

For more advanced tutorials on embeddings and VectorDB refer to my Github:

Data-Science-Python/VectorDB_Tutorial.ipynb at master · krishnayogik/Data-Science-Python
Data Science with Python. Contribute to krishnayogik/Data-Science-Python development by creating an account on GitHub.
github.com

Relationship Between Embeddings and VectorDB
Storage and Retrieval:

Embeddings are stored in VectorDBs to enable efficient retrieval based on similarity searches.
VectorDBs provide the infrastructure to manage and query large sets of embeddings.
Indexing:

VectorDBs use specialized indexing techniques to organize embeddings, making search operations efficient even with high-dimensional data.
Similarity Search:

One of the primary uses of embeddings is to find similar items (e.g., similar documents, images, or users). VectorDBs optimize this process by providing fast and scalable similarity search capabilities.
Applications:

Recommendation Systems: Embeddings representing users and items are stored in VectorDBs to generate personalized recommendations.
Search Engines: Embeddings of documents or images are indexed in VectorDBs for semantic search.
Anomaly Detection: Embeddings of normal behavior are stored in VectorDBs, and incoming data can be compared to detect anomalies based on similarity.
Example Workflow
Generate Embeddings:

Use a machine learning model to convert raw data (e.g., text, images) into embeddings.
Example: Use BERT to generate embeddings for text documents.
Store Embeddings in VectorDB:

Store the generated embeddings in a VectorDB for efficient management and retrieval.
Example: Use FAISS to index and store the embeddings.
Query and Retrieve Similar Items:

Perform similarity searches to find items that are close to a given query embedding.
Example: Given an embedding of a search query, retrieve the most similar documents from the VectorDB.
Embeddings and VectorDBs work hand in hand to enable efficient storage, management, and retrieval of high-dimensional vector data. Embeddings provide a powerful way to represent data in a numerical format, while VectorDBs offer the necessary tools to manage and search these embeddings at scale, making them crucial components in modern AI and machine learning applications.

